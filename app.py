import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin
import io

# ç”»é¢ã‚’åºƒãä½¿ã†è¨­å®š
st.set_page_config(page_title="ãƒã‚­ãƒ†ãƒƒã‚¯HP altæŠ½å‡ºãƒ„ãƒ¼ãƒ« v6", layout="wide")

st.title("ãƒã‚­ãƒ†ãƒƒã‚¯HPã€€è£½å“ãƒšãƒ¼ã‚¸altæŠ½å‡ºãƒ„ãƒ¼ãƒ« v6")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šé™¤å¤–ãƒªã‚¹ãƒˆã®ç®¡ç† ---
with st.sidebar:
    st.header("ğŸ›  é™¤å¤–URLãƒªã‚¹ãƒˆç®¡ç†")
    st.write("å‰Šé™¤ã—ãŸã„URLã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’1è¡Œãšã¤å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    # å‰Šé™¤ã—ãŸã„URLã‚’å¤§é‡ã«å…¥ã‚Œã‚‰ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢
    # heightã‚’æŒ‡å®šã—ã¦æ ã‚’å¤§ããã—ã¦ã„ã¾ã™
    exclude_text = st.text_area(
        "é™¤å¤–ãƒªã‚¹ãƒˆï¼ˆãƒ¡ãƒ¢å¸³ï¼‰", 
        value=st.session_state.get('exclude_list_raw', ""),
        height=400,
        placeholder="https://www.makitech.co.jp/index.html\n/support/\n/company/",
        help="ã“ã“ã«ç™»éŒ²ã•ã‚ŒãŸæ–‡å­—ã‚’å«ã‚€è¡Œã¯ã€æŠ½å‡ºçµæœã‹ã‚‰è‡ªå‹•çš„ã«å‰Šé™¤ã•ã‚Œã¾ã™ã€‚"
    )
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ã—ã¦ä¿æŒ
    st.session_state['exclude_list_raw'] = exclude_text
    exclude_list = [line.strip() for line in exclude_text.split("\n") if line.strip()]
    
    st.info(f"ç¾åœ¨ {len(exclude_list)} ä»¶ã®é™¤å¤–ãƒ«ãƒ¼ãƒ«ãŒé©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚")

    # ãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãƒœã‚¿ãƒ³
    if st.button("ãƒªã‚¹ãƒˆã‚’ã™ã¹ã¦ã‚¯ãƒªã‚¢"):
        st.session_state['exclude_list_raw'] = ""
        st.rerun()

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼šæŠ½å‡ºè¨­å®š ---
col1, col2 = st.columns([2, 1])
with col1:
    target_url = st.text_input("èª¿æŸ»å…ƒã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒšãƒ¼ã‚¸URL", placeholder="https://www.makitech.co.jp/conveyor/index-2.html")
with col2:
    st.write("") # ã‚¹ãƒšãƒ¼ã‚¹èª¿æ•´
    extract_btn = st.button("Step 1: ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹", use_container_width=True)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'extracted_df' not in st.session_state:
    st.session_state.extracted_df = None

# --- æŠ½å‡ºå‡¦ç† ---
if extract_btn:
    if not target_url:
        st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        with st.spinner("å…¨ãƒšãƒ¼ã‚¸ã‚’è©³ç´°ã«èª¿æŸ»ä¸­..."):
            try:
                headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
                res = requests.get(target_url, headers=headers)
                res.encoding = res.apparent_encoding
                soup = BeautifulSoup(res.text, 'html.parser')

                links = []
                for a in soup.find_all('a', href=True):
                    url = urljoin(target_url, a['href'])
                    if (".html" in url) and (url != target_url) and ("#" not in url):
                        if url not in links:
                            links.append(url)

                if not links:
                    st.warning("å¯¾è±¡ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    all_data = []
                    progress_bar = st.progress(0)
                    for i, link in enumerate(links):
                        time.sleep(0.3)
                        try:
                            r = requests.get(link, headers=headers, timeout=10)
                            r.encoding = r.apparent_encoding
                            ps = BeautifulSoup(r.text, 'html.parser')
                            
                            t_div = ps.find('div', class_='m-t-20 text-medium')
                            model = t_div.get_text(strip=True) if t_div else "æœªè¨­å®š"
                            
                            main = ps.find(id='contents') or ps.find(class_='l-main') or ps
                            alts = [img.get('alt', '').strip() for img in main.find_all('img') if img.get('alt')]
                            
                            row = {"å‹ç•ª": model, "URL": link, "Title": ps.title.string if ps.title else ""}
                            for idx, val in enumerate(alts):
                                row[f"alt {idx+1}"] = val
                            all_data.append(row)
                        except: continue
                        progress_bar.progress((i + 1) / len(links))
                    
                    st.session_state.extracted_df = pd.DataFrame(all_data)
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# --- çµæœã®è¡¨ç¤ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---
if st.session_state.extracted_df is not None:
    df_display = st.session_state.extracted_df.copy()
    
    # é™¤å¤–ãƒªã‚¹ãƒˆã«åŸºã¥ã„ã¦è¡Œã‚’å‰Šé™¤
    if exclude_list:
        for ex in exclude_list:
            df_display = df_display[~df_display['URL'].str.contains(ex, na=False)]
    
    st.divider()
    st.subheader(f"æŠ½å‡ºãƒ»ãƒ•ã‚£ãƒ«ã‚¿çµæœ ï¼ˆç¾åœ¨ {len(df_display)} è¡Œï¼‰")
    st.dataframe(df_display, use_container_width=True)

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df_display.to_excel(writer, index=False)
    
    st.download_button(
        label="Step 2: ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã®ã‚¨ã‚¯ã‚»ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=output.getvalue(),
        file_name="makitech_alt_list_final.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True
    )

    st.caption("â€»å·¦å´ã®é™¤å¤–ãƒªã‚¹ãƒˆã‚’æ›¸ãæ›ãˆã‚‹ã¨ã€å³åº§ã«ä¸Šã®è¡¨ã«åæ˜ ã•ã‚Œã¾ã™ã€‚")
